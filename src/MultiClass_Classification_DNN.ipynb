{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Multi-Class Classification: Optimization of Deep Neural Networks (DNN)**\n",
    "\n",
    "In this Jupyer Notebook, we will implement a multiple layer feed-forward neural network for a multi-class classification task.\n",
    "\n",
    "***\n",
    "\n",
    "## **1. Build a Deep Neural Network Model**\n",
    "\n",
    "First, we implement the `NeuralNetModel1` class. The model takes a $28\\times 28$ grey-scale image as input, and pass it through a deep neural network.\n",
    "\n",
    "The network has 2 hidden layers and 1 output layers, whose sizes are: 512 -> 512 -> 10. That is, the number of output classes is 10. The activation function for each hidden layer is `ReLU`.\n",
    "\n",
    "The input image is first passed through a `nn.Flatten()` layer so that a 2D tensor becomes 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetModel1, self).__init__()\n",
    "        self.flatten = nn.Flatten() # Use nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), # Input size is 28*28\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.Linear(512, 512), # 512 -> 512\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.Linear(512, 10), # 512 -> 10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # Call self.flatten()\n",
    "        logits = self.linear_relu_stack(x) # Call self.linear_relu_stack()\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([5, 28, 28])\n",
      "output size: torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "sample_input = torch.randn(5, 28, 28)\n",
    "print('input size:', sample_input.size())\n",
    "\n",
    "model1 = NeuralNetModel1()\n",
    "with torch.no_grad():\n",
    "    output = model1(sample_input)\n",
    "print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## **2. Use Dataloader**\n",
    "\n",
    "Next, we download the FashionMNIST dataset provided by PyTorch to the folder \"data\", which takes some time for the first time execution.\n",
    "We will use the `DataLoader` module to wrap the loaded training and test data, and then specify the `batch_size` correctly for both training and test dataloader.\n",
    "\n",
    "See <https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 16972754.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 206245.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:08<00:00, 537172.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 2257897.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True, # True\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # False\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size) # Specify data source and batch size correctly\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 60000\n",
      "Testing data size: 10000\n",
      "X size: torch.Size([64, 1, 28, 28])\n",
      "y size: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "print('Training data size:', len(training_data))\n",
    "print('Testing data size:', len(test_data))\n",
    "\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print('X size:', X.size())\n",
    "    print('y size:', y.size())\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "## **3. Define Loss and Optimizer**\n",
    "\n",
    "We will use `nn.CrossEntropyLoss()` as the loss function, and `torch.optim.SGD()` as the optimizer. We need to specify the arguments for `SGD()`, including the learning rate correctly.\n",
    "\n",
    "See <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html> and <https://pytorch.org/docs/stable/optim.html> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_sgd = torch.optim.SGD(model1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "<class 'torch.optim.sgd.SGD'>\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "print(loss_fn)\n",
    "print(type(optimizer_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "\n",
    "## **4. Implement Train and Test Functions**\n",
    "\n",
    "Now, we have to implement the code for training the model in `train()`, as well as implement the code for testing the model in `test()`. For the backpropagation step, we need to first zero out all gradients by calling `optimizer.zero_grad()` before carrying out `backward()` and `step()` to update parameters.\n",
    "\n",
    "In `test()`, we need to calculate the number of correct prediction in the current batch, and add it to the `correct` variable.\n",
    "Finally, we need to divide `correct` by the total number of test examples to obtain the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, verbose=True):\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X) # Get the prediction output from model\n",
    "        loss = loss_fn(pred, y) # compute loss by calling loss_fn()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # zero_grad()\n",
    "        loss.backward() # backward()\n",
    "        optimizer.step() # step()\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            current_step = i * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current_step:>5d}/{len(dataloader.dataset):>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X) # Similar to how it is computed in train()\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item() # Add the number of correct prediction in the current batch to `correct`\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc = correct / len(dataloader.dataset) # Use `correct` to compute accuracy\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will execute the following cell to start the training and testing loop after making sure that the cell containing the loss function and optimizers has already been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296701  [    0/60000]\n",
      "loss: 2.284246  [ 6400/60000]\n",
      "loss: 2.266165  [12800/60000]\n",
      "loss: 2.270359  [19200/60000]\n",
      "loss: 2.252516  [25600/60000]\n",
      "loss: 2.231632  [32000/60000]\n",
      "loss: 2.239763  [38400/60000]\n",
      "loss: 2.206202  [44800/60000]\n",
      "loss: 2.202750  [51200/60000]\n",
      "loss: 2.174662  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.171464 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.178694  [    0/60000]\n",
      "loss: 2.166120  [ 6400/60000]\n",
      "loss: 2.112745  [12800/60000]\n",
      "loss: 2.130643  [19200/60000]\n",
      "loss: 2.084808  [25600/60000]\n",
      "loss: 2.037292  [32000/60000]\n",
      "loss: 2.053999  [38400/60000]\n",
      "loss: 1.979528  [44800/60000]\n",
      "loss: 1.982393  [51200/60000]\n",
      "loss: 1.910791  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.913630 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.944621  [    0/60000]\n",
      "loss: 1.911706  [ 6400/60000]\n",
      "loss: 1.797971  [12800/60000]\n",
      "loss: 1.834718  [19200/60000]\n",
      "loss: 1.737230  [25600/60000]\n",
      "loss: 1.692371  [32000/60000]\n",
      "loss: 1.701045  [38400/60000]\n",
      "loss: 1.604594  [44800/60000]\n",
      "loss: 1.627077  [51200/60000]\n",
      "loss: 1.516714  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.542017 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.613817  [    0/60000]\n",
      "loss: 1.572235  [ 6400/60000]\n",
      "loss: 1.423317  [12800/60000]\n",
      "loss: 1.487492  [19200/60000]\n",
      "loss: 1.381456  [25600/60000]\n",
      "loss: 1.379046  [32000/60000]\n",
      "loss: 1.382812  [38400/60000]\n",
      "loss: 1.307113  [44800/60000]\n",
      "loss: 1.343914  [51200/60000]\n",
      "loss: 1.239240  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.270233 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.355618  [    0/60000]\n",
      "loss: 1.330860  [ 6400/60000]\n",
      "loss: 1.165463  [12800/60000]\n",
      "loss: 1.263089  [19200/60000]\n",
      "loss: 1.144845  [25600/60000]\n",
      "loss: 1.173153  [32000/60000]\n",
      "loss: 1.188248  [38400/60000]\n",
      "loss: 1.121184  [44800/60000]\n",
      "loss: 1.165405  [51200/60000]\n",
      "loss: 1.079876  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.102930 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.182019  [    0/60000]\n",
      "loss: 1.179150  [ 6400/60000]\n",
      "loss: 0.998185  [12800/60000]\n",
      "loss: 1.125664  [19200/60000]\n",
      "loss: 1.001181  [25600/60000]\n",
      "loss: 1.036263  [32000/60000]\n",
      "loss: 1.070050  [38400/60000]\n",
      "loss: 1.002466  [44800/60000]\n",
      "loss: 1.048712  [51200/60000]\n",
      "loss: 0.981322  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.995854 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.061846  [    0/60000]\n",
      "loss: 1.080970  [ 6400/60000]\n",
      "loss: 0.883552  [12800/60000]\n",
      "loss: 1.035262  [19200/60000]\n",
      "loss: 0.912065  [25600/60000]\n",
      "loss: 0.940016  [32000/60000]\n",
      "loss: 0.993520  [38400/60000]\n",
      "loss: 0.923600  [44800/60000]\n",
      "loss: 0.967148  [51200/60000]\n",
      "loss: 0.914717  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.922171 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.972424  [    0/60000]\n",
      "loss: 1.012110  [ 6400/60000]\n",
      "loss: 0.799891  [12800/60000]\n",
      "loss: 0.970487  [19200/60000]\n",
      "loss: 0.853134  [25600/60000]\n",
      "loss: 0.868749  [32000/60000]\n",
      "loss: 0.939845  [38400/60000]\n",
      "loss: 0.869769  [44800/60000]\n",
      "loss: 0.907700  [51200/60000]\n",
      "loss: 0.866192  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.868535 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.902862  [    0/60000]\n",
      "loss: 0.960342  [ 6400/60000]\n",
      "loss: 0.736596  [12800/60000]\n",
      "loss: 0.921305  [19200/60000]\n",
      "loss: 0.811513  [25600/60000]\n",
      "loss: 0.814860  [32000/60000]\n",
      "loss: 0.899155  [38400/60000]\n",
      "loss: 0.831842  [44800/60000]\n",
      "loss: 0.863127  [51200/60000]\n",
      "loss: 0.828609  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.827640 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.847125  [    0/60000]\n",
      "loss: 0.918837  [ 6400/60000]\n",
      "loss: 0.687079  [12800/60000]\n",
      "loss: 0.882682  [19200/60000]\n",
      "loss: 0.780198  [25600/60000]\n",
      "loss: 0.773582  [32000/60000]\n",
      "loss: 0.866388  [38400/60000]\n",
      "loss: 0.804114  [44800/60000]\n",
      "loss: 0.828847  [51200/60000]\n",
      "loss: 0.798178  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.795201 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetModel1() # Reset the model\n",
    "optimizer_sgd = torch.optim.SGD(model1.parameters(), lr=learning_rate) # Because the model1 is reset, the optimizer also needs redefined.\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model1, loss_fn, optimizer_sgd, verbose=True) # Use verbose=False to see less information\n",
    "    test_loop(test_loader, model1, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we train an ADAM optimizer. Note that the model needs be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 0.565680  [ 6400/60000]\n",
      "loss: 0.392925  [12800/60000]\n",
      "loss: 0.491291  [19200/60000]\n",
      "loss: 0.450433  [25600/60000]\n",
      "loss: 0.447974  [32000/60000]\n",
      "loss: 0.378937  [38400/60000]\n",
      "loss: 0.541005  [44800/60000]\n",
      "loss: 0.462502  [51200/60000]\n",
      "loss: 0.504917  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.424086 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.265505  [    0/60000]\n",
      "loss: 0.358143  [ 6400/60000]\n",
      "loss: 0.300393  [12800/60000]\n",
      "loss: 0.383802  [19200/60000]\n",
      "loss: 0.442644  [25600/60000]\n",
      "loss: 0.385706  [32000/60000]\n",
      "loss: 0.301127  [38400/60000]\n",
      "loss: 0.506399  [44800/60000]\n",
      "loss: 0.393916  [51200/60000]\n",
      "loss: 0.408847  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.394956 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.213341  [    0/60000]\n",
      "loss: 0.321439  [ 6400/60000]\n",
      "loss: 0.251663  [12800/60000]\n",
      "loss: 0.315905  [19200/60000]\n",
      "loss: 0.405041  [25600/60000]\n",
      "loss: 0.343355  [32000/60000]\n",
      "loss: 0.265248  [38400/60000]\n",
      "loss: 0.462261  [44800/60000]\n",
      "loss: 0.349393  [51200/60000]\n",
      "loss: 0.358563  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.373790 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.230239  [    0/60000]\n",
      "loss: 0.299979  [ 6400/60000]\n",
      "loss: 0.243942  [12800/60000]\n",
      "loss: 0.271627  [19200/60000]\n",
      "loss: 0.385186  [25600/60000]\n",
      "loss: 0.341051  [32000/60000]\n",
      "loss: 0.243705  [38400/60000]\n",
      "loss: 0.415004  [44800/60000]\n",
      "loss: 0.319980  [51200/60000]\n",
      "loss: 0.349157  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.358218 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.207198  [    0/60000]\n",
      "loss: 0.258332  [ 6400/60000]\n",
      "loss: 0.226630  [12800/60000]\n",
      "loss: 0.247008  [19200/60000]\n",
      "loss: 0.388421  [25600/60000]\n",
      "loss: 0.344426  [32000/60000]\n",
      "loss: 0.223778  [38400/60000]\n",
      "loss: 0.369507  [44800/60000]\n",
      "loss: 0.294492  [51200/60000]\n",
      "loss: 0.344114  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.349748 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.197794  [    0/60000]\n",
      "loss: 0.236495  [ 6400/60000]\n",
      "loss: 0.194530  [12800/60000]\n",
      "loss: 0.236059  [19200/60000]\n",
      "loss: 0.303656  [25600/60000]\n",
      "loss: 0.317825  [32000/60000]\n",
      "loss: 0.179116  [38400/60000]\n",
      "loss: 0.356194  [44800/60000]\n",
      "loss: 0.312754  [51200/60000]\n",
      "loss: 0.331944  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.340147 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.191527  [    0/60000]\n",
      "loss: 0.200340  [ 6400/60000]\n",
      "loss: 0.216254  [12800/60000]\n",
      "loss: 0.221639  [19200/60000]\n",
      "loss: 0.287645  [25600/60000]\n",
      "loss: 0.289815  [32000/60000]\n",
      "loss: 0.228135  [38400/60000]\n",
      "loss: 0.318764  [44800/60000]\n",
      "loss: 0.289847  [51200/60000]\n",
      "loss: 0.349094  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.347619 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.161636  [    0/60000]\n",
      "loss: 0.180277  [ 6400/60000]\n",
      "loss: 0.207360  [12800/60000]\n",
      "loss: 0.222036  [19200/60000]\n",
      "loss: 0.331237  [25600/60000]\n",
      "loss: 0.285589  [32000/60000]\n",
      "loss: 0.236223  [38400/60000]\n",
      "loss: 0.305225  [44800/60000]\n",
      "loss: 0.248444  [51200/60000]\n",
      "loss: 0.330139  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg loss: 0.346575 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.160567  [    0/60000]\n",
      "loss: 0.187903  [ 6400/60000]\n",
      "loss: 0.183838  [12800/60000]\n",
      "loss: 0.206751  [19200/60000]\n",
      "loss: 0.278054  [25600/60000]\n",
      "loss: 0.256366  [32000/60000]\n",
      "loss: 0.208828  [38400/60000]\n",
      "loss: 0.242921  [44800/60000]\n",
      "loss: 0.306692  [51200/60000]\n",
      "loss: 0.247035  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.348457 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.126176  [    0/60000]\n",
      "loss: 0.147469  [ 6400/60000]\n",
      "loss: 0.203282  [12800/60000]\n",
      "loss: 0.184430  [19200/60000]\n",
      "loss: 0.276049  [25600/60000]\n",
      "loss: 0.217365  [32000/60000]\n",
      "loss: 0.244404  [38400/60000]\n",
      "loss: 0.218883  [44800/60000]\n",
      "loss: 0.243984  [51200/60000]\n",
      "loss: 0.261898  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.352222 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetModel1() # Reset the model\n",
    "optimizer_adam = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model1, loss_fn, optimizer_adam, verbose=True) # Use verbose=False to see less information\n",
    "    test_loop(test_loader, model1, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "## **5. Add Batchnorm and Dropout**\n",
    "\n",
    "We use `torch.nn.BatchNorm1d()` and `nn.Dropout()` after the ReLU activation of each hidden layer. `Batchnorm1d()` takes the size of previous activation as input. `Dropout()` takes the probability of dropout as input.\n",
    "\n",
    "For more information, see <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> and <https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel2(nn.Module):\n",
    "    def __init__(self, dropout = 0.1): # Note the additional dropout parameter here\n",
    "        \"\"\"\n",
    "        :param dropout: float, the probability of dropout\n",
    "        \"\"\"\n",
    "        super(NeuralNetModel2, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.BatchNorm1d(512), # Batchnorm\n",
    "            nn.Dropout(dropout), # Dropout, use the `dropout` parameter\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(), # ReLU\n",
    "            torch.nn.BatchNorm1d(512), # Batchnorm\n",
    "            nn.Dropout(dropout), # Dropout, use the `dropout` parameter\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following cell, we test with different `dropout` rates, and observe how that affects the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.625102  [    0/60000]\n",
      "loss: 0.610666  [ 6400/60000]\n",
      "loss: 0.552281  [12800/60000]\n",
      "loss: 0.565507  [19200/60000]\n",
      "loss: 0.617299  [25600/60000]\n",
      "loss: 0.525321  [32000/60000]\n",
      "loss: 0.482823  [38400/60000]\n",
      "loss: 0.575578  [44800/60000]\n",
      "loss: 0.607219  [51200/60000]\n",
      "loss: 0.560870  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.538211 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.416563  [    0/60000]\n",
      "loss: 0.445116  [ 6400/60000]\n",
      "loss: 0.357440  [12800/60000]\n",
      "loss: 0.490875  [19200/60000]\n",
      "loss: 0.521853  [25600/60000]\n",
      "loss: 0.494577  [32000/60000]\n",
      "loss: 0.425976  [38400/60000]\n",
      "loss: 0.566340  [44800/60000]\n",
      "loss: 0.519707  [51200/60000]\n",
      "loss: 0.460394  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.515284 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.276398  [    0/60000]\n",
      "loss: 0.480249  [ 6400/60000]\n",
      "loss: 0.375430  [12800/60000]\n",
      "loss: 0.468507  [19200/60000]\n",
      "loss: 0.329319  [25600/60000]\n",
      "loss: 0.494466  [32000/60000]\n",
      "loss: 0.424919  [38400/60000]\n",
      "loss: 0.554629  [44800/60000]\n",
      "loss: 0.624207  [51200/60000]\n",
      "loss: 0.388514  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.491575 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.308995  [    0/60000]\n",
      "loss: 0.377803  [ 6400/60000]\n",
      "loss: 0.345858  [12800/60000]\n",
      "loss: 0.452209  [19200/60000]\n",
      "loss: 0.333726  [25600/60000]\n",
      "loss: 0.417759  [32000/60000]\n",
      "loss: 0.345204  [38400/60000]\n",
      "loss: 0.565347  [44800/60000]\n",
      "loss: 0.544042  [51200/60000]\n",
      "loss: 0.472514  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.468470 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.283980  [    0/60000]\n",
      "loss: 0.396865  [ 6400/60000]\n",
      "loss: 0.282520  [12800/60000]\n",
      "loss: 0.365121  [19200/60000]\n",
      "loss: 0.389858  [25600/60000]\n",
      "loss: 0.459842  [32000/60000]\n",
      "loss: 0.348608  [38400/60000]\n",
      "loss: 0.649985  [44800/60000]\n",
      "loss: 0.561075  [51200/60000]\n",
      "loss: 0.444328  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.458723 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.312675  [    0/60000]\n",
      "loss: 0.431800  [ 6400/60000]\n",
      "loss: 0.355958  [12800/60000]\n",
      "loss: 0.411420  [19200/60000]\n",
      "loss: 0.367876  [25600/60000]\n",
      "loss: 0.468287  [32000/60000]\n",
      "loss: 0.354574  [38400/60000]\n",
      "loss: 0.509907  [44800/60000]\n",
      "loss: 0.486595  [51200/60000]\n",
      "loss: 0.354329  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.473103 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.288054  [    0/60000]\n",
      "loss: 0.408226  [ 6400/60000]\n",
      "loss: 0.279562  [12800/60000]\n",
      "loss: 0.400285  [19200/60000]\n",
      "loss: 0.430245  [25600/60000]\n",
      "loss: 0.453274  [32000/60000]\n",
      "loss: 0.422989  [38400/60000]\n",
      "loss: 0.586387  [44800/60000]\n",
      "loss: 0.401151  [51200/60000]\n",
      "loss: 0.505889  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.463012 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.338994  [    0/60000]\n",
      "loss: 0.331214  [ 6400/60000]\n",
      "loss: 0.313018  [12800/60000]\n",
      "loss: 0.384604  [19200/60000]\n",
      "loss: 0.412354  [25600/60000]\n",
      "loss: 0.451596  [32000/60000]\n",
      "loss: 0.350305  [38400/60000]\n",
      "loss: 0.413538  [44800/60000]\n",
      "loss: 0.470410  [51200/60000]\n",
      "loss: 0.602401  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.445997 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.222109  [    0/60000]\n",
      "loss: 0.332042  [ 6400/60000]\n",
      "loss: 0.302147  [12800/60000]\n",
      "loss: 0.463183  [19200/60000]\n",
      "loss: 0.378250  [25600/60000]\n",
      "loss: 0.398547  [32000/60000]\n",
      "loss: 0.384119  [38400/60000]\n",
      "loss: 0.590312  [44800/60000]\n",
      "loss: 0.504772  [51200/60000]\n",
      "loss: 0.530748  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.439901 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.290174  [    0/60000]\n",
      "loss: 0.497595  [ 6400/60000]\n",
      "loss: 0.231312  [12800/60000]\n",
      "loss: 0.483797  [19200/60000]\n",
      "loss: 0.396523  [25600/60000]\n",
      "loss: 0.452941  [32000/60000]\n",
      "loss: 0.451867  [38400/60000]\n",
      "loss: 0.620077  [44800/60000]\n",
      "loss: 0.537186  [51200/60000]\n",
      "loss: 0.430366  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.443431 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model2 = NeuralNetModel2(dropout=0.5) # Call NeuralNetModel2() with different dropout values\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate) # Can also try Adam/SGD optimizer\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model2, loss_fn, optimizer, verbose=True) # Use verbose=False to see less information\n",
    "    test_loop(test_loader, model2, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
